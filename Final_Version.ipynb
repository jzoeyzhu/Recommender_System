{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpO365o4OT60"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz\n",
        "!tar xzf spark-3.3.3-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HcO40wSOYG8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.3-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLSuhU0xOZ9R"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,BooleanType,DoubleType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jCi2f3L8t0M",
        "outputId": "bae8a761-3484-4b37-f2d1-dffa480d2918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVFrNJlRCV-q",
        "outputId": "2367a976-5cc6-4aec-9109-095c1829c178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.10/dist-packages (1.1.3)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.4)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi4P0JtG9Jb-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from surprise import Reader, Dataset\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from surprise import SVD\n",
        "from surprise import accuracy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from surprise.model_selection import cross_validate, GridSearchCV\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import Dataset, Reader, SVD, SVDpp\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
        "from sklearn.model_selection import GridSearchCV as sk_GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from pyspark.sql import SparkSession, Row, Window\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, IndexToString, StringIndexerModel, Word2Vec, Tokenizer, CountVectorizer, MinHashLSH\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id, udf, max, min, avg, mean, count, explode, row_number, collect_list, desc, lit, broadcast\n",
        "from pyspark.sql.types import FloatType, StructType, StructField, StringType\n",
        "from pyspark.ml.recommendation import ALS, ALSModel\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESmrz0UmM-gG"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-jVK3H8M9u0"
      },
      "outputs": [],
      "source": [
        "# Create a Spark session with a meaningful app name\n",
        "spark = SparkSession.builder.appName(\"RecommendationSystemApp\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a PySpark DataFrame\n",
        "data_path = '/content/drive/MyDrive/Data_Preprocessing/final_training_dataset.csv'\n",
        "df = spark.read.option(\"header\", \"true\") \\\n",
        "               .option(\"inferSchema\", \"true\") \\\n",
        "               .option(\"quote\", \"\\\"\") \\\n",
        "               .option(\"escape\", \"\\\"\") \\\n",
        "               .csv(data_path)\n",
        "\n",
        "# remove unnessary columns\n",
        "columns_to_drop = ['review_text', 'sentiment', 'detect']\n",
        "df = df.drop(*columns_to_drop)\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_df_spark, test_df_spark = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Convert user_id and business_id to numerical indices\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_indexed\").fit(df) for column in ([\"user_id\", \"business_id\"])]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "indexer_model = pipeline.fit(train_df_spark)\n",
        "train_df_spark = indexer_model.transform(train_df_spark)\n",
        "test_df_spark = indexer_model.transform(test_df_spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yluHZHkc9Ujl",
        "outputId": "62d23aeb-0a63-4fe7-dea9-0c27d1135577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+------+--------------------+------------------+---------------+-------------------+\n",
            "|         business_id|             user_id|rating|     cleaned_reviews|   sentiment_score|user_id_indexed|business_id_indexed|\n",
            "+--------------------+--------------------+------+--------------------+------------------+---------------+-------------------+\n",
            "|604092d865e4ba058...|10301301358122957...|   3.0|       shrimp scampi|0.9999940395355225|         4894.0|             3638.0|\n",
            "|604092d865e4ba058...|11326994618515516...|   5.0|great time ate lo...|0.9999945163726807|        18985.0|             3638.0|\n",
            "|604094167cd8bf130...|10014988986459009...|   5.0|got ta fight chee...|0.9999940395355225|         8588.0|              694.0|\n",
            "+--------------------+--------------------+------+--------------------+------------------+---------------+-------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_df_spark.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoUiRVsiO753"
      },
      "outputs": [],
      "source": [
        "# first write it to a parquet format, then read it as a pandas dataframe; faster than direct convert\n",
        "train_df_spark.write.mode(\"overwrite\").parquet(\"/content/drive/MyDrive/Data_Preprocessing/training.parquet\")\n",
        "test_df_spark.write.mode(\"overwrite\").parquet(\"/content/drive/MyDrive/Data_Preprocessing/testing.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udxBcnHTNL3U"
      },
      "outputs": [],
      "source": [
        "# convert to pandas dataframe for model fitting\n",
        "train_df = pd.read_parquet(\"/content/drive/MyDrive/Data_Preprocessing/training.parquet\")\n",
        "test_df = pd.read_parquet(\"/content/drive/MyDrive/Data_Preprocessing/testing.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_kdGQoQAYKo"
      },
      "source": [
        "## Model With Scikit-Learn (Hybrid Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3zt9th8BMzr"
      },
      "source": [
        "**Collaborative Filtering (SVD) with Surprise:**\n",
        "\n",
        "This part primarily uses the Surprise library for collaborative filtering with SVD. Since Surprise is well-suited for collaborative filtering tasks and we're performing grid search for hyperparameter tuning, there's no strong reason to switch this part to Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_9ZNSGw92hJ",
        "outputId": "19da3fc8-9a5d-46cf-e500-d26046b26c59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVDpp at 0x79065cfe7700>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "collab_features = ['user_id', 'business_id']\n",
        "content_features = ['cleaned_reviews', 'sentiment_score']\n",
        "target = 'rating'\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Load train and test data into the Dataset format\n",
        "train_data = Dataset.load_from_df(train_df[['user_id', 'business_id', 'rating']], reader)\n",
        "test_data = Dataset.load_from_df(test_df[['user_id', 'business_id', 'rating']], reader)\n",
        "\n",
        "# Specify the parameter grid for GridSearchCV\n",
        "param_grid = {'n_epochs': [10, 20, 30], 'lr_all': [0.002, 0.005, 0.01], 'reg_all': [0.1, 0.4]}\n",
        "\n",
        "# Create GridSearchCV with the correct metric 'rmse'\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5)\n",
        "gs.fit(train_data)  # Fit only on the trainset\n",
        "\n",
        "# Get the best RMSE score and the corresponding parameters\n",
        "best_rmse = gs.best_score['rmse']\n",
        "best_params = gs.best_params['rmse']\n",
        "\n",
        "# Instantiate the SVD model with the best parameters\n",
        "collab_model = SVDpp(n_epochs=best_params['n_epochs'], lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
        "\n",
        "# Fit collab_model on the trainset\n",
        "trainset = train_data.build_full_trainset()  # Build full trainset\n",
        "collab_model.fit(trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnZiKoGu_GoI",
        "outputId": "a03215f3-0206-477a-d3d1-071b96eca249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.8176\n",
            "Best Parameters: {'n_epochs': 20, 'lr_all': 0.01, 'reg_all': 0.4}\n"
          ]
        }
      ],
      "source": [
        "print('Best RMSE: {:.4f}'.format(best_rmse))\n",
        "print('Best Parameters:', best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P1zLiNQBjUC"
      },
      "source": [
        "**Content-Based Model (TF-IDF and Ridge Regression):**\n",
        "\n",
        "This part involves using scikit-learn's pipeline and grid search for hyperparameter tuning with TF-IDF and Ridge Regression. Since these are common machine learning tasks, we choose to keep this part as is unless it is a very large dataset where Spark's distributed capabilities could be advantageous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCfSWoo_iAx",
        "outputId": "90107744-82c2-4934-f501-3c4c0977172c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('text',\n",
              "                                                  TfidfVectorizer(max_features=1500),\n",
              "                                                  'cleaned_reviews'),\n",
              "                                                 ('numeric', 'passthrough',\n",
              "                                                  ['sentiment_score'])])),\n",
              "                ('ridge', Ridge(alpha=10.0, random_state=42))])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;text&#x27;,\n",
              "                                                  TfidfVectorizer(max_features=1500),\n",
              "                                                  &#x27;cleaned_reviews&#x27;),\n",
              "                                                 (&#x27;numeric&#x27;, &#x27;passthrough&#x27;,\n",
              "                                                  [&#x27;sentiment_score&#x27;])])),\n",
              "                (&#x27;ridge&#x27;, Ridge(alpha=10.0, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;text&#x27;,\n",
              "                                                  TfidfVectorizer(max_features=1500),\n",
              "                                                  &#x27;cleaned_reviews&#x27;),\n",
              "                                                 (&#x27;numeric&#x27;, &#x27;passthrough&#x27;,\n",
              "                                                  [&#x27;sentiment_score&#x27;])])),\n",
              "                (&#x27;ridge&#x27;, Ridge(alpha=10.0, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;text&#x27;, TfidfVectorizer(max_features=1500),\n",
              "                                 &#x27;cleaned_reviews&#x27;),\n",
              "                                (&#x27;numeric&#x27;, &#x27;passthrough&#x27;,\n",
              "                                 [&#x27;sentiment_score&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>cleaned_reviews</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1500)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numeric</label><div class=\"sk-toggleable__content\"><pre>[&#x27;sentiment_score&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=10.0, random_state=42)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline # there are two pipelines, spark and sklearn\n",
        "\n",
        "# Data splitting\n",
        "X_train, y_train = train_df[['cleaned_reviews', 'sentiment_score']], train_df['rating']\n",
        "X_test, y_test = test_df[['cleaned_reviews', 'sentiment_score']], test_df['rating']\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('text', TfidfVectorizer(), 'cleaned_reviews'),\n",
        "        ('numeric', 'passthrough', ['sentiment_score'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Full pipeline with Ridge regression\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('ridge', Ridge(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'preprocessor__text__max_features': [500, 1000, 1500],\n",
        "    'preprocessor__text__ngram_range': [(1, 1), (1, 2)],\n",
        "    'ridge__alpha': [0.1, 1.0, 10.0]\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = sk_GridSearchCV(pipeline, param_grid, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Model training and hyperparameter tuning\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_score = -grid_search.best_score_  # Convert to positive mean squared error\n",
        "content_model = grid_search.best_estimator_\n",
        "content_model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXrOxf2FBBC6",
        "outputId": "fa81bb8e-17a7-4140-9ef4-68d7f8f8cbc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'preprocessor__text__max_features': 1500, 'preprocessor__text__ngram_range': (1, 1), 'ridge__alpha': 10.0}\n",
            "Best score (MSE): 0.3687917954347598\n"
          ]
        }
      ],
      "source": [
        "print(f'Best parameters: {best_params}')\n",
        "print(f'Best score (MSE): {best_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRVBRpBNBtLR"
      },
      "source": [
        "**Combining Predictions and Calculating RMSE:**\n",
        "\n",
        "This part combines predictions from both models and calculates the final RMSE. Since it involves working with Pandas and NumPy arrays, and the computations are not distributed.\n",
        "\n",
        "In summary, if the dataset is large and we need to scale collaborative filtering or content-based modeling tasks, we might consider using Spark for those parts. However, for smaller datasets, the current implementation using scikit-learn and Surprise libraries should be sufficient and straightforward. It's a trade-off between scalability and simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLO9dlxlBTWb",
        "outputId": "ee956d8c-524c-4f08-d201-978de8c01e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final RMSE: 0.6811783847706706\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a collab_model and content_model defined\n",
        "testset = test_data.build_full_trainset().build_testset()\n",
        "# Collaborative Filtering Model Predictions\n",
        "col_predictions = collab_model.test(testset)\n",
        "\n",
        "# Extract estimated ratings\n",
        "col_predictions = [pred.est for pred in col_predictions]\n",
        "\n",
        "# Assuming you have a content-based model (content_model) defined\n",
        "con_predictions = np.array(content_model.predict(X_test))\n",
        "\n",
        "# Combine Predictions\n",
        "final_predictions = np.mean([col_predictions, con_predictions], axis=0)\n",
        "\n",
        "# Calculate RMSE\n",
        "final_rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
        "print(f'Final RMSE: {final_rmse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1903GnLBAjUj"
      },
      "source": [
        "## Modelling with Spark (ALS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITlnahA4BDWY"
      },
      "outputs": [],
      "source": [
        "# tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_reviews\", outputCol=\"words\")\n",
        "tokenized_df = tokenizer.transform(train_df_spark)\n",
        "\n",
        "# Word2Vec model\n",
        "word2Vec = Word2Vec(vectorSize=130, minCount=0, inputCol=\"words\", outputCol=\"word2vec_features\")\n",
        "word2Vec_model = word2Vec.fit(tokenized_df)\n",
        "train_word2vec_df = word2Vec_model.transform(tokenized_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU5qwYJ-CjJt"
      },
      "outputs": [],
      "source": [
        "# Add unique IDs to each row\n",
        "train_word2vec_df = train_word2vec_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "# Self-join to calculate similarities\n",
        "joined_df = train_word2vec_df.alias(\"df1\").join(train_word2vec_df.alias(\"df2\"), \"user_id\").filter(\"df1.id < df2.id\")\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    # Convert to NumPy arrays\n",
        "    vec1_array = np.array(vec1.toArray())\n",
        "    vec2_array = np.array(vec2.toArray())\n",
        "\n",
        "    return float(np.dot(vec1_array, vec2_array) / (np.linalg.norm(vec1_array) * np.linalg.norm(vec2_array)))\n",
        "\n",
        "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
        "\n",
        "# Apply the UDF\n",
        "similarity_df = joined_df.withColumn(\"similarity\", cosine_similarity_udf(\"df1.word2vec_features\", \"df2.word2vec_features\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNcbip1rC0qf"
      },
      "outputs": [],
      "source": [
        "# Normalize the similarity scores\n",
        "max_score = similarity_df.agg(max(col(\"similarity\")).alias(\"max_score\")).collect()[0][\"max_score\"]\n",
        "min_score = similarity_df.agg(min(col(\"similarity\")).alias(\"min_score\")).collect()[0][\"min_score\"]\n",
        "\n",
        "similarity_df = similarity_df.withColumn(\"normalized_similarity\",\n",
        "                                         (col(\"similarity\") - min_score) / (max_score - min_score))\n",
        "global_avg_similarity = similarity_df.agg({\"normalized_similarity\": \"avg\"}).collect()[0][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3JAh0yJC-P2"
      },
      "outputs": [],
      "source": [
        "# Fill missing values in user profiles with the global average\n",
        "user_profile_df = similarity_df.groupBy(\"user_id\").agg({\"normalized_similarity\": \"avg\"}) \\\n",
        "                                .na.fill(global_avg_similarity)\n",
        "\n",
        "# Rename the column for clarity\n",
        "user_profile_df = user_profile_df.withColumnRenamed(\"avg(normalized_similarity)\", \"user_similarity_avg\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RszFLNo6DDbo"
      },
      "outputs": [],
      "source": [
        "# Join user profile data with the original reviews dataset\n",
        "enriched_df = train_df_spark.join(user_profile_df, train_df_spark[\"user_id\"] == user_profile_df[\"user_id\"])\n",
        "\n",
        "# Aggregate user profile information for each restaurant\n",
        "restaurant_profile_df = enriched_df.groupBy(\"business_id\").agg({\"user_similarity_avg\": \"avg\"})\n",
        "\n",
        "# Rename the column for clarity and fill missing values\n",
        "restaurant_profile_df = restaurant_profile_df.withColumnRenamed(\"avg(user_similarity_avg)\", \"restaurant_similarity_profile\") \\\n",
        "                                             .na.fill(global_avg_similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWd713-bDMuf"
      },
      "outputs": [],
      "source": [
        "als_model_df = train_df_spark.join(user_profile_df, \"user_id\").join(restaurant_profile_df, \"business_id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQO77xtQDXal"
      },
      "outputs": [],
      "source": [
        "#### DON RUN THIS, THE MODEL HAS BEEN TRAINED ALREADY , THIS WILL TAKE TOO LONG TO FINISH\n",
        "\n",
        "als_model_df = als_model_df.withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
        "\n",
        "# Define the ALS model with nonnegative constraint and coldStartStrategy\n",
        "als = ALS(userCol=\"user_id_indexed\", itemCol=\"business_id_indexed\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
        "\n",
        "# Adjust the ParamGridBuilder with a wider range of regParam and potentially lower ranks\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(als.rank, [5, 10, 20]).addGrid(als.maxIter, [10, 15]).addGrid(als.regParam, [0.05, 0.1, 0.2]).build()\n",
        "\n",
        "# Define an evaluator with RMSE metric\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "\n",
        "# CrossValidator with 5-fold cross-validation\n",
        "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
        "\n",
        "# Fit the model\n",
        "model = cv.fit(als_model_df)\n",
        "\n",
        "# Extract the best model\n",
        "best_model = model.bestModel\n",
        "\n",
        "# Optional: Print out the best parameters\n",
        "print(\"Best rank:\", best_model.rank)\n",
        "print(\"Best maxIter:\", best_model._java_obj.parent().getMaxIter())\n",
        "print(\"Best regParam:\", best_model._java_obj.parent().getRegParam())\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/cs631alsmodels_new\"\n",
        "best_model.write().overwrite().save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7j9y2zrDvwP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.recommendation import ALSModel\n",
        "model_path = \"/content/drive/MyDrive/cs631alsmodels_new\"\n",
        "als_model = ALSModel.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSl_r8exE4md",
        "outputId": "17cadd8c-c929-4a99-d208-96cdcdd0d582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root-mean-square error = 1.3283079852708082\n"
          ]
        }
      ],
      "source": [
        "test_model_df = test_df_spark.withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
        "predictions = als_model.transform(test_model_df)\n",
        "predictions = predictions.na.drop(subset=[\"prediction\"])\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root-mean-square error = \" + str(rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvnxuvR7NSV1"
      },
      "source": [
        "## Generating recommender function\n",
        "For the followig parts, I experimented with both Spark and Pandas implementations. But I observed similar performance speeds in both cases. The reasons could be:\n",
        "1. Extensive Use of UDFs (User-Defined Functions): The content and collaborative models are derived from different frameworks. To integrate these models effectively, numerous UDFs were required. UDFs in Spark, while flexible, can be less efficient than built-in Spark functions due to the added serialization and deserialization overhead, especially when dealing with complex operations.\n",
        "\n",
        "2. Conversions Between Pandas and Spark DataFrames: Despite efforts to minimize conversions, some degree of data transfer between Pandas and Spark DataFrames was inevitable. These conversions are computationally costly, as they involve serialization and deserialization of data, leading to additional processing time that could negate some of Spark's performance advantages.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRv52vHwjJcB",
        "outputId": "45707634-7574-498f-a2ee-07f49d36d500"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[business_id: string, user_id: string, rating: string, cleaned_reviews: string, sentiment_score: double, user_id_indexed: double, business_id_indexed: double, words: array<string>, features: vector, hashes: array<vector>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# how to handle the deployment problem?\n",
        "\n",
        "agg_reviews = train_df.groupby('business_id')['cleaned_reviews'].apply(' '.join)\n",
        "agg_sentiments = train_df.groupby('business_id')['sentiment_score'].mean()\n",
        "\n",
        "\n",
        "def predict_collaborative_filtering(collab_model, user_id, all_restaurant_ids, rating_threshold=4.5):\n",
        "    predictions = []\n",
        "    for restaurant_id in all_restaurant_ids:\n",
        "        # Predict the rating for each restaurant\n",
        "        prediction = collab_model.predict(user_id, restaurant_id)\n",
        "        if prediction.est > rating_threshold:\n",
        "            predictions.append((restaurant_id, prediction.est))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def refine_with_content_model(user_id, potential_restaurants, content_model, restaurant_dict, batch_size=50):\n",
        "    refined_recommendations = []\n",
        "    batch_data = []\n",
        "    batch_restaurants = []\n",
        "\n",
        "    for restaurant_id, collab_pred_rating in potential_restaurants:\n",
        "        restaurant_data = restaurant_dict.get(restaurant_id)\n",
        "\n",
        "        if restaurant_data is not None and not restaurant_data.empty:\n",
        "            batch_data.append(restaurant_data)\n",
        "            batch_restaurants.append((restaurant_id, collab_pred_rating))\n",
        "\n",
        "        # Process the batch when the batch size is reached or at the end of the list\n",
        "        if len(batch_data) == batch_size or restaurant_id == potential_restaurants[-1][0]:\n",
        "            if batch_data:\n",
        "                # Concatenate the batch data\n",
        "                batch_df = pd.concat(batch_data, ignore_index=True)\n",
        "\n",
        "                # Predict using the content model for the batch\n",
        "                batch_predictions = content_model.predict(batch_df)\n",
        "\n",
        "                # Iterate over the batch predictions and combine with collaborative ratings\n",
        "                for (restaurant_id, collab_pred_rating), content_pred_rating in zip(batch_restaurants, batch_predictions):\n",
        "                    final_rating = (collab_pred_rating*0.5 + content_pred_rating*0.5)\n",
        "                    refined_recommendations.append((restaurant_id, final_rating))\n",
        "\n",
        "            # Reset batch data and restaurants list for the next batch\n",
        "            batch_data = []\n",
        "            batch_restaurants = []\n",
        "\n",
        "    return refined_recommendations\n",
        "\n",
        "\n",
        "def get_top_rated_restaurant_per_user(train_df):\n",
        "    # Group by user and aggregate with a custom function to find the top-rated restaurant\n",
        "    def get_top_restaurant(group):\n",
        "        top_rated = group.sort_values(by='rating', ascending=False).head(1)\n",
        "        return top_rated['business_id'].values[0]\n",
        "\n",
        "    top_rated_restaurants = train_df.groupby('user_id').apply(get_top_restaurant)\n",
        "\n",
        "    return top_rated_restaurants.to_dict()\n",
        "\n",
        "\n",
        "def get_similar_restaurants(business_id, transformed_df, mh_model):\n",
        "    key = transformed_df.filter(transformed_df['business_id'] == business_id).head()\n",
        "    if key:\n",
        "        similar = mh_model.approxSimilarityJoin(transformed_df, transformed_df, 1.0, distCol=\"JaccardDistance\") \\\n",
        "            .filter(col(\"datasetA.business_id\") == key['business_id']) \\\n",
        "            .select(col(\"datasetB.business_id\").alias(\"similar_business_id\"), col(\"JaccardDistance\")) \\\n",
        "            .dropDuplicates([\"similar_business_id\"]) \\\n",
        "            .orderBy(\"JaccardDistance\", ascending=True)\n",
        "\n",
        "        # Convert to Pandas DataFrame and then to dictionary\n",
        "        similar_pd = similar.toPandas()\n",
        "        similarity_dict = dict(zip(similar_pd['similar_business_id'], similar_pd['JaccardDistance']))\n",
        "        return similarity_dict\n",
        "    else:\n",
        "        print(\"Business ID not found in the dataset.\")\n",
        "        return None\n",
        "\n",
        "# Preprocess the data once\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=30000)\n",
        "cv_model = cv.fit(tokenized_df)\n",
        "featurized_df = cv_model.transform(tokenized_df)\n",
        "\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
        "mh_model = mh.fit(featurized_df)\n",
        "transformed_df = mh_model.transform(featurized_df)\n",
        "transformed_df.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzooGn1oWhcZ"
      },
      "outputs": [],
      "source": [
        "restaurant_dict = {key: group[['cleaned_reviews', 'sentiment_score']] for key, group in train_df.groupby('business_id')}\n",
        "top_rated_restaurants_all = get_top_rated_restaurant_per_user(train_df)\n",
        "all_restaurant_ids = train_df['business_id'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJWN643qv1c-"
      },
      "outputs": [],
      "source": [
        "# how to improve precision@k and recall@k?\n",
        "\n",
        "# train_df['rating'] = train_df['rating'].astype('float')\n",
        "\n",
        "# # Calculate C, the mean rating across all restaurants\n",
        "# C = train_df['rating'].mean()\n",
        "\n",
        "# # Choose a value for m, the minimum reviews required\n",
        "# m = train_df['business_id'].value_counts().quantile(0.90)\n",
        "\n",
        "# # Calculate v (number of reviews) and R (average rating) for each restaurant\n",
        "# v = train_df.groupby('business_id')['rating'].count()\n",
        "# R = train_df.groupby('business_id')['rating'].mean()\n",
        "\n",
        "# # Calculate weighted rating\n",
        "# weighted_rating = (v / (v + m)) * R + (m / (v + m)) * C\n",
        "\n",
        "# # Convert to a dictionary\n",
        "# item_popularity = weighted_rating.to_dict()\n",
        "\n",
        "def re_rank_recommendations(user_id, refined_recommendations, top_rated_restaurants_all,\n",
        "                            transformed_df, mh_model, alpha=0.2):\n",
        "    if alpha == 0:\n",
        "      return refined_recommendations\n",
        "\n",
        "    re_ranked_recommendations = []\n",
        "\n",
        "    top_rated_restaurant = top_rated_restaurants_all[user_id]\n",
        "\n",
        "    # Get similarity scores for the top-rated restaurant\n",
        "    similarity_scores = get_similar_restaurants(top_rated_restaurant, transformed_df, mh_model)\n",
        "\n",
        "\n",
        "    for restaurant_id, combined_rating in refined_recommendations:\n",
        "        similarity_score = similarity_scores.get(restaurant_id, 0)  # Default to 0 if no similarity score\n",
        "\n",
        "        # Adjust the final score to include similarity\n",
        "        final_score = (1 - alpha) * combined_rating + (alpha) *  (1 - similarity_score)\n",
        "        re_ranked_recommendations.append((restaurant_id, final_score))\n",
        "\n",
        "    return re_ranked_recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw_S6lixm_x_"
      },
      "outputs": [],
      "source": [
        "def get_als_recommendations(user_id, indexer_model, als_model, k=100):\n",
        "    # Convert user_id to user_id_indexed\n",
        "    user_df = spark.createDataFrame([Row(user_id=user_id)])\n",
        "    indexed_user_df = indexer_model.transform(user_df)\n",
        "    user_id_indexed = indexed_user_df.select(\"user_id_indexed\").collect()[0][\"user_id_indexed\"]\n",
        "\n",
        "    # Get recommendations for the user\n",
        "    recs = als_model.recommendForUserSubset(spark.createDataFrame([Row(user_id_indexed=user_id_indexed)]), k)\n",
        "\n",
        "    # Extract the StringIndexerModel for business_id from the indexer_model pipeline\n",
        "    business_id_indexer_model = [stage for stage in indexer_model.stages if isinstance(stage, StringIndexerModel) and stage.getOutputCol() == \"business_id_indexed\"][0]\n",
        "\n",
        "    # Create an IndexToString transformer\n",
        "    converter = IndexToString(inputCol=\"business_id_indexed\", outputCol=\"business_id\", labels=business_id_indexer_model.labels)\n",
        "\n",
        "    # Transform the recommendations DataFrame to extract business_id_indexed and convert them to original business IDs\n",
        "    recs_transformed = recs.select(\"user_id_indexed\", explode(\"recommendations\").alias(\"recommendation\"))\n",
        "    business_ids_df = recs_transformed.select(\"user_id_indexed\", col(\"recommendation.business_id_indexed\").alias(\"business_id_indexed\"))\n",
        "\n",
        "    original_ids_df = converter.transform(business_ids_df)\n",
        "\n",
        "    als_rankings = {row['business_id']: idx + 1 for idx, row in enumerate(original_ids_df.collect())}\n",
        "    return als_rankings\n",
        "\n",
        "\n",
        "def precompute_most_popular_items(train_df, k=20):\n",
        "    # Calculate popularity as number of visits * average rating\n",
        "    train_df['rating'] = train_df['rating'].astype('float')\n",
        "    popularity_df = train_df.groupby('business_id').agg(\n",
        "        num_visits=('business_id', 'count'),\n",
        "        avg_rating=('rating', 'mean')).reset_index()\n",
        "    popularity_df['popularity_score'] = popularity_df['num_visits'] * popularity_df['avg_rating']\n",
        "\n",
        "    # Sort and get top k most popular business IDs\n",
        "    most_popular_business_ids = popularity_df.sort_values('popularity_score', ascending=False)['business_id'][:k].tolist()\n",
        "    return most_popular_business_ids\n",
        "\n",
        "def generate_k_recommendations(user_id, all_restaurant_ids, indexer_model, als_model, collab_model, content_model,\n",
        "                               train_df, restaurant_dict, transformed_df, mh_model, most_popular_business_ids,\n",
        "                               k=20, rating_threshold=4.5, alpha=0.2, include_als=False):\n",
        "\n",
        "    if user_id not in train_df['user_id'].unique():\n",
        "      # Cold-start scenario: Recommend most popular items\n",
        "      return most_popular_business_ids[:k]\n",
        "\n",
        "    if not include_als:\n",
        "        # Get collaborative filtering recommendations\n",
        "        potential_restaurants = predict_collaborative_filtering(collab_model, user_id, all_restaurant_ids, rating_threshold)\n",
        "        # Refine recommendations using the content model\n",
        "        content_refined_recommendations = refine_with_content_model(user_id, potential_restaurants, content_model, restaurant_dict, batch_size=1000)\n",
        "        # Use re_rank_recommendations to get refined recommendations\n",
        "        re_ranked_recommendations = re_rank_recommendations(user_id, content_refined_recommendations, top_rated_restaurants_all, transformed_df, mh_model, alpha)\n",
        "        return re_ranked_recommendations[:k]\n",
        "\n",
        "    # Get ALS model recommendations with rankings\n",
        "    als_rankings = get_als_recommendations(user_id, indexer_model, als_model, 50)\n",
        "\n",
        "    # Get collaborative filtering recommendations\n",
        "    potential_restaurants =  predict_collaborative_filtering(collab_model, user_id, all_restaurant_ids, rating_threshold)\n",
        "    #print(collab_recommendations)\n",
        "\n",
        "    # Refine recommendations using the content model\n",
        "    content_refined_recommendations = refine_with_content_model(user_id, potential_restaurants, content_model, restaurant_dict, batch_size=1000)\n",
        "    #print(collab_recommendations)\n",
        "    # Use re_rank_recommendations to get refined recommendations\n",
        "    re_ranked_recommendations = re_rank_recommendations(user_id, content_refined_recommendations, top_rated_restaurants_all, transformed_df, mh_model, alpha)\n",
        "\n",
        "    # Incorporate ALS rankings into the final score\n",
        "    final_recommendations = []\n",
        "    for restaurant_id, combined_score in re_ranked_recommendations:\n",
        "        als_rank = als_rankings.get(restaurant_id, 51)  # Default to a low rank if not in top 50\n",
        "        final_score = combined_score / als_rank  # Higher ALS rank results in a higher final score\n",
        "        final_recommendations.append((restaurant_id, final_score))\n",
        "\n",
        "    # Sort by final score and return top k recommendations\n",
        "    final_recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "    return final_recommendations[:k]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUd72nimI0UM",
        "outputId": "d4a34ed5-04e7-4c25-a71a-7bd99a3f1c72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['604245d6b9a6829e686e8c2a',\n",
              " '6043ad17b81264dfa846c9ea',\n",
              " '60415f44c6fcf1fddba13088',\n",
              " '604153f5c6fcf1fddba12bd2',\n",
              " '604242b42e57ebdea29c91f6',\n",
              " '604158f57dfa7f1871835dd4',\n",
              " '6056030897d555cc6fb0d03b',\n",
              " '604bdd2a21f213251c55e437',\n",
              " '60424db77dfa7f187183bd4d',\n",
              " '60458535af942d7ea319b5f0',\n",
              " '604162547cd8bf1303625275',\n",
              " '60418e07c6fcf1fddba13f2f',\n",
              " '604156f62e57ebdea29c37dc',\n",
              " '60418a3b2e57ebdea29c4f21',\n",
              " '6040ce24c6fcf1fddba0f0e0',\n",
              " '6040c0517cd8bf130361fe41',\n",
              " '604222e85fd99145bc6227dc',\n",
              " '604c0a7c21f213251c55e58a',\n",
              " '6040fe28c6fcf1fddba108dd',\n",
              " '604159ca2e57ebdea29c39a3']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Precompute the list once\n",
        "most_popular_business_ids = precompute_most_popular_items(train_df)\n",
        "most_popular_business_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "tmuXL-ZEUlND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = '111581047256694238215'\n",
        "get_als_recommendations(user_id, indexer_model, als_model, k=20)"
      ],
      "metadata": {
        "id": "fKnCk8mHUjWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUQJKuJvvx08",
        "outputId": "3add79ef-3aa6-4f1d-f60f-8d32344545db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "generate_k_recommendations(user_id, all_restaurant_ids, indexer_model, als_model, collab_model, content_model,\n",
        "                               train_df, restaurant_dict, item_popularity, transformed_df, mh_model, most_popular_business_ids,\n",
        "                               k=20, rating_threshold=4.5, alpha=0.3, beta=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "wTAlCOX-yUU3",
        "outputId": "13de8a67-a05c-4195-e807-ce08657dd37c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   business_id                user_id  rating  \\\n",
              "2551  605687e3f69c7b117807008c  115222930312185568212     4.0   \n",
              "\n",
              "                                        cleaned_reviews  sentiment_score  \\\n",
              "2551  wife salad czech caprese salad version main co...         0.999994   \n",
              "\n",
              "      user_id_indexed  business_id_indexed  \n",
              "2551           7710.0               2637.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6428acc7-943a-46fd-941e-e19dd5a07a8f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>user_id_indexed</th>\n",
              "      <th>business_id_indexed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2551</th>\n",
              "      <td>605687e3f69c7b117807008c</td>\n",
              "      <td>115222930312185568212</td>\n",
              "      <td>4.0</td>\n",
              "      <td>wife salad czech caprese salad version main co...</td>\n",
              "      <td>0.999994</td>\n",
              "      <td>7710.0</td>\n",
              "      <td>2637.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6428acc7-943a-46fd-941e-e19dd5a07a8f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6428acc7-943a-46fd-941e-e19dd5a07a8f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6428acc7-943a-46fd-941e-e19dd5a07a8f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "test_df[test_df['user_id'] == user_id]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df['user_id'] == user_id]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "VALzMl4uDTTR",
        "outputId": "a505fe38-71f5-4bd0-cc47-ae787dde9c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    business_id                user_id  rating  \\\n",
              "27095  604a88b0b1a0aaee3eefbc99  105638162784132957228     2.0   \n",
              "\n",
              "                                         cleaned_reviews  sentiment_score  \\\n",
              "27095  granted breakfast screw country fried steak eg...         0.000309   \n",
              "\n",
              "       user_id_indexed  business_id_indexed  \n",
              "27095          12924.0              18195.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-70dd7e0b-095e-412f-bf59-5c480d309a03\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>user_id_indexed</th>\n",
              "      <th>business_id_indexed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27095</th>\n",
              "      <td>604a88b0b1a0aaee3eefbc99</td>\n",
              "      <td>105638162784132957228</td>\n",
              "      <td>2.0</td>\n",
              "      <td>granted breakfast screw country fried steak eg...</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>12924.0</td>\n",
              "      <td>18195.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70dd7e0b-095e-412f-bf59-5c480d309a03')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-70dd7e0b-095e-412f-bf59-5c480d309a03 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-70dd7e0b-095e-412f-bf59-5c480d309a03');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "als_rankings = get_als_recommendations(user_id, indexer_model, als_model, 50)\n",
        "# collab_recommendations = predict_collaborative_filtering(collab_model, user_id, all_restaurant_ids, 1)\n",
        "# content_refined_recommendations = refine_with_content_model(user_id, collab_recommendations, content_model, restaurant_dict, batch_size=1000)\n",
        "# refined_recommendations = re_rank_recommendations(user_id, content_refined_recommendations, item_popularity, top_rated_restaurants_all, transformed_df, mh_model, 0.3, 0)\n",
        "# final_recommendations = []\n",
        "# for restaurant_id, combined_score in refined_recommendations:\n",
        "#     als_rank = als_rankings.get(restaurant_id, 51)  # Default to a low rank if not in top 50\n",
        "#     final_score = combined_score / als_rank  # Higher ALS rank results in a higher final score\n",
        "#     final_recommendations.append((restaurant_id, final_score))\n",
        "\n",
        "# Sort by final score and return top k recommendations\n",
        "final_recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "final_recommendations = final_recommendations[:50]"
      ],
      "metadata": {
        "id": "ICLk7jeADcb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNkfFdlr_WCX",
        "outputId": "0627a853-39d4-4e96-8fe2-235a0d8329ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value not found in any tuple.\n"
          ]
        }
      ],
      "source": [
        "# Value to check\n",
        "value_to_check = '604a88b0b1a0aaee3eefbc99'\n",
        "\n",
        "# Function to find the tuple containing the value\n",
        "def find_tuple_with_value(tuples_list, value):\n",
        "    for item in tuples_list:\n",
        "        if value in item:\n",
        "            return item\n",
        "    return None  # Return None if the value is not found in any tuple\n",
        "\n",
        "# Find the tuple\n",
        "result = find_tuple_with_value(final_recommendations, value_to_check)\n",
        "\n",
        "if result:\n",
        "    print(f\"Found the value in tuple: {result}\")\n",
        "else:\n",
        "    print(\"Value not found in any tuple.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeHQ5kako07-"
      },
      "source": [
        "## Performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_recall_at_k(content_model, collab_model, als_model, test_df, test_df_spark, all_restaurant_ids,\n",
        "                          indexer_model, transformed_df, mh_model, most_popular_business_ids,\n",
        "                          k=10, threshold=4, content=False, collab=True, als=False, alpha=0.3, use_min_hash=False):\n",
        "    '''Return precision and recall at k metrics for each user.'''\n",
        "    user_est_true = defaultdict(list)\n",
        "    remove_threshold = False # indicate whether to remove the threshold for the estimated predictions\n",
        "    if content and not collab:\n",
        "        # Generate predictions from the content model\n",
        "        X_test = test_df[['cleaned_reviews', 'sentiment_score']]\n",
        "        predictions = content_model.predict(X_test)\n",
        "        for uid, true_r, est in zip(test_df['user_id'], test_df['rating'], predictions):\n",
        "            user_est_true[uid].append((est,float(true_r)))\n",
        "\n",
        "    if collab and not content:\n",
        "        # Generate predictions from the collaborative model\n",
        "        test_data = Dataset.load_from_df(test_df[['user_id', 'business_id', 'rating']], reader)\n",
        "        testset = test_data.build_full_trainset().build_testset()\n",
        "        predictions = collab_model.test(testset)\n",
        "        for uid, _, true_r, est, _ in predictions:\n",
        "            user_est_true[uid].append((est, float(true_r)))\n",
        "\n",
        "    if content and collab and not als:\n",
        "        if not use_min_hash:\n",
        "            # Generate predictions from both models and combine them\n",
        "            # take 30 samples\n",
        "            X_test = test_df[['cleaned_reviews', 'sentiment_score']]\n",
        "            content_predictions = content_model.predict(X_test)\n",
        "            test_data = Dataset.load_from_df(test_df[['user_id', 'business_id', 'rating']], reader)\n",
        "            testset = test_data.build_full_trainset().build_testset()\n",
        "            collab_predictions = collab_model.test(testset)\n",
        "            for uid, true_r, est_content, est_collab in zip(test_df['user_id'], test_df['rating'], content_predictions, [pred.est for pred in collab_predictions]):\n",
        "                combined_est = 0.5 * est_content + 0.5 * est_collab\n",
        "                user_est_true[uid].append((combined_est, float(true_r)))\n",
        "        else:\n",
        "            remove_threshold = True\n",
        "            unique_users_train = set(train_df['user_id'].unique())\n",
        "            unique_businesses_train = set(train_df['business_id'].unique())\n",
        "            filtered_test_df = test_df[test_df['user_id'].isin(unique_users_train) & test_df['business_id'].isin(unique_businesses_train)]\n",
        "\n",
        "            X_test = filtered_test_df[['cleaned_reviews', 'sentiment_score']]\n",
        "            content_predictions = content_model.predict(X_test)\n",
        "            test_data = Dataset.load_from_df(filtered_test_df[['user_id', 'business_id', 'rating']], reader)\n",
        "            testset = test_data.build_full_trainset().build_testset()\n",
        "            collab_predictions = collab_model.test(testset)\n",
        "            temp_dict = defaultdict(list)\n",
        "            for (uid, restaurant_id, true_r), est_content, est_collab in zip(filtered_test_df[['user_id', 'business_id', 'rating']].itertuples(index=False), content_predictions, [pred.est for pred in collab_predictions]):\n",
        "                combined_est = 0.5 * est_content + 0.5 * est_collab\n",
        "                temp_dict[uid].append((restaurant_id, combined_est, float(true_r)))\n",
        "\n",
        "            for uid, ratings in temp_dict.items():\n",
        "                top_rated_restaurant = top_rated_restaurants_all.get(uid)\n",
        "                if top_rated_restaurant:\n",
        "                    similarity_scores = get_similar_restaurants(top_rated_restaurant, transformed_df, mh_model)\n",
        "\n",
        "                    for i, (restaurant_id, combined_rating, true_r) in enumerate(ratings):\n",
        "                        similarity_score = similarity_scores.get(restaurant_id, 0)  # Default to 0 if no similarity score\n",
        "\n",
        "                        # Adjust the final score to include similarity\n",
        "                        final_score = (1 - alpha) * combined_rating + (alpha) *  (1 - similarity_score)\n",
        "\n",
        "                        # Update the list with the new combined rating\n",
        "                        ratings[i] = (final_score, true_r)\n",
        "                # Update the dictionary with the new ratings\n",
        "                user_est_true[uid] = ratings\n",
        "\n",
        "\n",
        "\n",
        "    if als and not content and not collab:\n",
        "        remove_threshold = True\n",
        "        test_model_df = test_df_spark.withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
        "        # Generate predictions from the ALS model\n",
        "        predictions = als_model.transform(test_df_spark)\n",
        "        predictions = predictions.na.drop(subset=[\"prediction\"])\n",
        "        for row in predictions.collect():\n",
        "            uid, true_r, est = row['user_id'], row['rating'], row['prediction']\n",
        "            true_r = float(true_r) if isinstance(true_r, str) else true_r\n",
        "            est = float(est) if isinstance(est, str) else est\n",
        "            user_est_true[uid].append((est, true_r))\n",
        "\n",
        "\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        if remove_threshold:\n",
        "            # Number of recommended items in top k\n",
        "            n_rec_k = sum((est >= 0) for (est, _) in user_ratings[:k])\n",
        "\n",
        "            # Number of relevant and recommended items in top k\n",
        "            n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= 0))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "        else:\n",
        "            # Number of recommended items in top k\n",
        "            n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "            # Number of relevant and recommended items in top k\n",
        "            n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                                  for (est, true_r) in user_ratings[:k])\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
        "\n",
        "    return precisions, recalls\n"
      ],
      "metadata": {
        "id": "nJ-nSMJTsgmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ALS Model\n"
      ],
      "metadata": {
        "id": "gROJtI1O6lng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_to_ave = []\n",
        "rec_to_ave = []\n",
        "\n",
        "precisions, recalls = precision_recall_at_k(content_model, collab_model, als_model, test_df, test_df_spark, all_restaurant_ids,\n",
        "                          indexer_model, transformed_df, mh_model, most_popular_business_ids,\n",
        "                          k=10, threshold=4, content=False, collab=False, als=True, alpha=0.3)\n",
        "\n",
        "# Precision and recall can then be averaged over all users\n",
        "\n",
        "prec_to_ave.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
        "rec_to_ave.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
        "\n",
        "\n",
        "precision_average = sum(prec_to_ave)/len(prec_to_ave)\n",
        "recall_average = sum(rec_to_ave)/len(prec_to_ave)\n",
        "\n",
        "print(\"Precision and Recall averages are {0} and {1}, respectively\".format(precision_average, recall_average))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmbtZHiu6oZa",
        "outputId": "7e07c97e-931e-43dc-fded-cc9e95611f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision and Recall averages are 0.8817234410194698 and 1.0, respectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collaborative Model"
      ],
      "metadata": {
        "id": "x3rbg52dswqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_to_ave = []\n",
        "rec_to_ave = []\n",
        "\n",
        "precisions, recalls =  precision_recall_at_k(content_model, collab_model, als_model, test_df, test_df_spark, all_restaurant_ids,\n",
        "                          indexer_model, transformed_df, mh_model, most_popular_business_ids,\n",
        "                          k=10, threshold=4, content=False, collab=True, als=False, alpha=0.3)\n",
        "\n",
        "# Precision and recall can then be averaged over all users\n",
        "\n",
        "prec_to_ave.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
        "rec_to_ave.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
        "\n",
        "\n",
        "precision_average = sum(prec_to_ave)/len(prec_to_ave)\n",
        "recall_average = sum(rec_to_ave)/len(prec_to_ave)\n",
        "\n",
        "print(\"Precision and Recall averages are {0} and {1}, respectively\".format(precision_average, recall_average))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41R7z4NosvHP",
        "outputId": "23a5de78-a573-4487-e06b-d062ec01c364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision and Recall averages are 0.8949618199172772 and 0.9740080602396862, respectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Content Model"
      ],
      "metadata": {
        "id": "MtWzY52ks30o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_to_ave = []\n",
        "rec_to_ave = []\n",
        "\n",
        "precisions, recalls = precision_recall_at_k(content_model, collab_model, als_model, test_df, test_df_spark, all_restaurant_ids,\n",
        "                          indexer_model, transformed_df, mh_model, most_popular_business_ids,\n",
        "                          k=10, threshold=4, content=True, collab=False, als=False, alpha=0.3)\n",
        "\n",
        "# Precision and recall can then be averaged over all users\n",
        "\n",
        "prec_to_ave.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
        "rec_to_ave.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
        "\n",
        "\n",
        "precision_average = sum(prec_to_ave)/len(prec_to_ave)\n",
        "recall_average = sum(rec_to_ave)/len(prec_to_ave)\n",
        "\n",
        "print(\"Precision and Recall averages are {0} and {1}, respectively\".format(precision_average, recall_average))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbExXBfJs2_q",
        "outputId": "5d326088-cefd-4f42-fdbc-beabe0f9ee7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision and Recall averages are 0.9424919927882072 and 0.9865007954183901, respectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The precision and recall seems higher for the content model. The test is impractical for evaluating a content model, as it depends on user reviews that are only available post-experience. An effective recommendation system should be capable of generating recommendations with minimal user input, typically just the user ID. To achieve this, integrating both collaborative and content-based models is advisable, leveraging the strengths of each to provide more accurate and timely recommendations."
      ],
      "metadata": {
        "id": "95SdOnBsxpmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Content + Collaborative (hybird Model)"
      ],
      "metadata": {
        "id": "do4fyHU_tjtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_to_ave = []\n",
        "rec_to_ave = []\n",
        "\n",
        "precisions, recalls = precision_recall_at_k(content_model, collab_model, als_model, test_df, test_df_spark, all_restaurant_ids,\n",
        "                          indexer_model, transformed_df, mh_model, most_popular_business_ids,\n",
        "                          k=10, threshold=4, content=True, collab=True, als=False, alpha=0)\n",
        "\n",
        "# Precision and recall can then be averaged over all users\n",
        "\n",
        "prec_to_ave.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
        "rec_to_ave.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
        "\n",
        "\n",
        "precision_average = sum(prec_to_ave)/len(prec_to_ave)\n",
        "recall_average = sum(rec_to_ave)/len(prec_to_ave)\n",
        "\n",
        "print(\"Precision and Recall averages are {0} and {1}, respectively\".format(precision_average, recall_average))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7TUQlxbtsYL",
        "outputId": "57d73291-a72b-458d-9b37-2379066679bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision and Recall averages are 0.9398832325803378 and 0.9917847067557535, respectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Content + Collaborative (Hybrid Model) + Min-Hash Similarity"
      ],
      "metadata": {
        "id": "-blyW4_ut1Wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prec_to_ave = []\n",
        "rec_to_ave = []\n",
        "\n",
        "precisions, recalls = precision_recall_at_k(content_model, collab_model, als_model, test_df, test_df_spark, all_restaurant_ids,\n",
        "                          indexer_model, transformed_df, mh_model, most_popular_business_ids,\n",
        "                          k=10, threshold=4, content=True, collab=True, als=False, alpha=0.2, use_min_hash=True)\n",
        "\n",
        "# Precision and recall can then be averaged over all users\n",
        "\n",
        "prec_to_ave.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
        "rec_to_ave.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
        "\n",
        "\n",
        "precision_average = sum(prec_to_ave)/len(prec_to_ave)\n",
        "recall_average = sum(rec_to_ave)/len(prec_to_ave)\n",
        "\n",
        "print(\"Precision and Recall averages are {0} and {1}, respectively\".format(precision_average, recall_average))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3qSbOW2v46H",
        "outputId": "8768d31a-ac0c-4ee0-fe67-2d4d2546483b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision and Recall averages are 0.8944723618090452 and 1.0, respectively\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the common users between train and test dataset."
      ],
      "metadata": {
        "id": "H0H7kS8ZtoBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LYOGdqXUqG0",
        "outputId": "19568084-3a2d-4f9b-c8e1-1d3131237d73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9479"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "user_id=train_df['user_id'].unique()\n",
        "user_id_test=test_df['user_id'].unique()\n",
        "len(set(user_id_test) & set(user_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9eDKx6a6VXb"
      },
      "outputs": [],
      "source": [
        "unique_users_train = set(train_df['user_id'].unique())\n",
        "unique_businesses_train = set(train_df['business_id'].unique())\n",
        "filtered_test_df = test_df[test_df['user_id'].isin(unique_users_train) & test_df['business_id'].isin(unique_businesses_train)]\n",
        "\n",
        "\n",
        "common_user_ids_df = train_df_spark.select('user_id').intersect(test_df_spark.select('user_id'))\n",
        "\n",
        "# Collecting the user IDs as a list\n",
        "common_user_ids_list = [row['user_id'] for row in common_user_ids_df.collect()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming"
      ],
      "metadata": {
        "id": "fU5t_v2Q9AEA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FaTQPuo1f4L"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.3-bin-hadoop3\"\n",
        "\n",
        "# import findspark\n",
        "# findspark.init()\n",
        "\n",
        "# from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "\n",
        "# This function creates SparkContext and StreamingContext\n",
        "# Do not change this function\n",
        "# Create a global variable for StreamingContext\n",
        "ssc = None\n",
        "\n",
        "# This function creates SparkContext and StreamingContext\n",
        "def initStreamingContext(sc, batch_duration=1):\n",
        "    global ssc\n",
        "    try:\n",
        "        # Stop the existing StreamingContext if it's not already terminated\n",
        "        if ssc is not None and not ssc.awaitTerminationOrTimeout(0):\n",
        "            ssc.stop(stopSparkContext=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error stopping existing StreamingContext: {e}\")\n",
        "\n",
        "    # Create a new StreamingContext\n",
        "    ssc = StreamingContext(sc, batch_duration)\n",
        "    return ssc\n",
        "\n",
        "# Call initStreamingContext\n",
        "ssc = initStreamingContext(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoK5RwaMCM_T"
      },
      "source": [
        "using Spark Streaming to process batches of randomly selected user IDs and generate recommendations for each user in each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsDYCVq6ZVLb",
        "outputId": "c952e9e0-47d7-471a-93b2-5da68a58abf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating recommendations for user 106234677322088610538\n",
            "Recommended business IDs: [('604092d865e4ba0588bb3ed5', 4.42293193293108), ('604094167cd8bf130361e4c6', 4.58039938378396), ('6040941b65e4ba0588bb3fc2', 4.427734264403102), ('604094449d953d1f97fa098a', 4.642523839534148), ('604094b365e4ba0588bb4004', 4.472968898546133), ('604094fb9d953d1f97fa09ee', 4.703966710314411), ('6040968fc6fcf1fddba0cf97', 4.6545952669206905), ('6040970365e4ba0588bb40ae', 4.3844996858283105), ('604097397cd8bf130361e5e5', 4.672037353372808), ('604097599d953d1f97fa0ac1', 4.544010693089707), ('6040977bc6fcf1fddba0cfed', 4.390187356938998), ('604098967cd8bf130361e755', 4.472438107193817), ('604098b3c6fcf1fddba0d0ec', 4.398122937307589), ('6040995465e4ba0588bb4214', 4.262226068559842), ('60409a03c6fcf1fddba0d1cb', 4.3030675045777365), ('60409b67c6fcf1fddba0d2f3', 4.655207735899008), ('60409c587cd8bf130361e9c9', 4.369906683020638), ('60409c79c6fcf1fddba0d36c', 4.335488841484212), ('60409d467cd8bf130361ea39', 4.474750604499703), ('60409d5665e4ba0588bb4533', 4.555581110672717)]\n",
            "Generating recommendations for user 115257142540307196287\n",
            "Recommended business IDs: [('604092d865e4ba0588bb3ed5', 4.274549678980429), ('604094167cd8bf130361e4c6', 4.378724427574458), ('6040941b65e4ba0588bb3fc2', 4.203353601557832), ('604094449d953d1f97fa098a', 4.450119378403917), ('604094b365e4ba0588bb4004', 4.225086535449705), ('604094fb9d953d1f97fa09ee', 4.474805889669961), ('6040968fc6fcf1fddba0cf97', 4.4632338032340275), ('6040970365e4ba0588bb40ae', 4.165455791543067), ('604097397cd8bf130361e5e5', 4.41041622796293), ('604097599d953d1f97fa0ac1', 4.326338056168204), ('6040977bc6fcf1fddba0cfed', 4.174585554346092), ('604098967cd8bf130361e755', 4.2481231207000665), ('604098b3c6fcf1fddba0d0ec', 4.267325749425458), ('6040995465e4ba0588bb4214', 4.0408815658065524), ('60409a03c6fcf1fddba0d1cb', 4.101543418356566), ('60409b67c6fcf1fddba0d2f3', 4.41158095610359), ('60409c587cd8bf130361e9c9', 4.135399924525524), ('60409c79c6fcf1fddba0d36c', 4.140447441312508), ('60409d467cd8bf130361ea39', 4.226462683537097), ('60409d5665e4ba0588bb4533', 4.330038844477819)]\n",
            "Generating recommendations for user 105634284377021694146\n",
            "Recommended business IDs: [('604092d865e4ba0588bb3ed5', 4.4377148907548225), ('604094167cd8bf130361e4c6', 4.647122530216107), ('6040941b65e4ba0588bb3fc2', 4.484993272241603), ('604094449d953d1f97fa098a', 4.694060861437855), ('604094b365e4ba0588bb4004', 4.545108029984705), ('604094fb9d953d1f97fa09ee', 4.766579589774213), ('6040968fc6fcf1fddba0cf97', 4.724442184148714), ('6040970365e4ba0588bb40ae', 4.442170712693159), ('604097397cd8bf130361e5e5', 4.736163360933187), ('604097599d953d1f97fa0ac1', 4.614265178167033), ('6040977bc6fcf1fddba0cfed', 4.44264634003515), ('604098967cd8bf130361e755', 4.534318212383897), ('604098b3c6fcf1fddba0d0ec', 4.443729484133449), ('6040995465e4ba0588bb4214', 4.3481707117234425), ('60409a03c6fcf1fddba0d1cb', 4.3277960266377935), ('60409b67c6fcf1fddba0d2f3', 4.717451792436293), ('60409c587cd8bf130361e9c9', 4.457351529320031), ('60409c79c6fcf1fddba0d36c', 4.447166328789399), ('60409d467cd8bf130361ea39', 4.54681381557897), ('60409d5665e4ba0588bb4533', 4.633349938228443)]\n",
            "Generating recommendations for user 100274129111249521459\n",
            "Recommended business IDs: [('604092d865e4ba0588bb3ed5', 4.425533539840739), ('604094167cd8bf130361e4c6', 4.61649532434426), ('6040941b65e4ba0588bb3fc2', 4.439001384012075), ('604094449d953d1f97fa098a', 4.655282817104858), ('604094b365e4ba0588bb4004', 4.466349492043755), ('604094fb9d953d1f97fa09ee', 4.712940787294949), ('6040968fc6fcf1fddba0cf97', 4.646826187827536), ('6040970365e4ba0588bb40ae', 4.386382273434624), ('604097397cd8bf130361e5e5', 4.64252868183849), ('604097599d953d1f97fa0ac1', 4.566557408764588), ('6040977bc6fcf1fddba0cfed', 4.411651301995942), ('604098967cd8bf130361e755', 4.48197852810853), ('604098b3c6fcf1fddba0d0ec', 4.428108612177985), ('6040995465e4ba0588bb4214', 4.286415048663476), ('60409a03c6fcf1fddba0d1cb', 4.2861051081137616), ('60409b67c6fcf1fddba0d2f3', 4.6393615373786865), ('60409c587cd8bf130361e9c9', 4.384144475330924), ('60409c79c6fcf1fddba0d36c', 4.370179265086092), ('60409d467cd8bf130361ea39', 4.502111259247516), ('60409d5665e4ba0588bb4533', 4.564272101676542)]\n",
            "Generating recommendations for user 107928871382391547537\n",
            "Recommended business IDs: [('604092d865e4ba0588bb3ed5', 4.431492254715465), ('604094167cd8bf130361e4c6', 4.54993178581144), ('6040941b65e4ba0588bb3fc2', 4.39937610595151), ('604094449d953d1f97fa098a', 4.586550902315733), ('604094b365e4ba0588bb4004', 4.447662126939068), ('604094fb9d953d1f97fa09ee', 4.651086769698666), ('6040968fc6fcf1fddba0cf97', 4.639610121168281), ('6040970365e4ba0588bb40ae', 4.367044936336724), ('604097397cd8bf130361e5e5', 4.603192132709768), ('604097599d953d1f97fa0ac1', 4.52111189905207), ('6040977bc6fcf1fddba0cfed', 4.335206782165647), ('604098967cd8bf130361e755', 4.429675015582156), ('604098b3c6fcf1fddba0d0ec', 4.398466045627124), ('6040995465e4ba0588bb4214', 4.240049906124297), ('60409a03c6fcf1fddba0d1cb', 4.27561964655289), ('60409b67c6fcf1fddba0d2f3', 4.599354994329307), ('60409c587cd8bf130361e9c9', 4.339959639900105), ('60409c79c6fcf1fddba0d36c', 4.344218320996104), ('60409d467cd8bf130361ea39', 4.485402012186174), ('60409d5665e4ba0588bb4533', 4.518882184113938)]\n",
            "Generating recommendations for user 109095265144580682697\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "# Randomly choose 30 user IDs every second and create a DStream\n",
        "import random\n",
        "\n",
        "ssc = initStreamingContext(sc)\n",
        "def get_random_user_ids(df, num_samples=30):\n",
        "    unique_user_ids = df['user_id'].unique()\n",
        "    num_unique_users = len(unique_user_ids)\n",
        "\n",
        "    if num_unique_users < num_samples:\n",
        "        print(f\"Warning: Not enough unique user IDs for sampling. Available unique user IDs: {num_unique_users}\")\n",
        "        num_samples = num_unique_users  # Reduce the number of samples to the number of unique users\n",
        "\n",
        "    if num_unique_users > 0:\n",
        "        return random.sample(list(unique_user_ids), num_samples)\n",
        "    else:\n",
        "        print(\"Error: No unique user IDs found.\")\n",
        "        return []\n",
        "\n",
        "# Use the function to create a DStream\n",
        "stream = ssc.queueStream([get_random_user_ids(test_df, 30) for _ in range(1000)])\n",
        "\n",
        "# Function to calculate precision and recall for a batch of user IDs\n",
        "def generate_recommedations(rdd):\n",
        "    user_ids = rdd.collect()\n",
        "    for user_id in user_ids:\n",
        "        print(\"Generating recommendations for user {0}\".format(user_id))\n",
        "        # Use the recommendation function to get recommended business IDs\n",
        "        recommended_business_ids = generate_k_recommendations(user_id, all_restaurant_ids, indexer_model, als_model, collab_model, content_model,\n",
        "                               train_df, restaurant_dict, transformed_df, mh_model, most_popular_business_ids,\n",
        "                               k=20, rating_threshold=1, alpha=0, include_als=False)\n",
        "        print(\"Recommended business IDs: {0}\".format(recommended_business_ids))\n",
        "\n",
        "# Process each batch of user IDs and calculate metrics\n",
        "stream.foreachRDD(generate_recommedations)\n",
        "\n",
        "# Start the Spark Streaming Context\n",
        "ssc.start()\n",
        "time.sleep(15)\n",
        "ssc.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMWqN9tMMONE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ESmrz0UmM-gG",
        "D_kdGQoQAYKo",
        "1903GnLBAjUj",
        "tmuXL-ZEUlND",
        "gROJtI1O6lng",
        "x3rbg52dswqe",
        "MtWzY52ks30o",
        "do4fyHU_tjtm"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
